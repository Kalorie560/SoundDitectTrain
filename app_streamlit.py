"""
SoundDitect - Streamlit Application
Simple, reliable real-time and offline audio anomaly detection

This replaces the complex JavaScript frontend with a simple, robust Streamlit interface
while maintaining all functionality from the original FastAPI backend.
"""

import streamlit as st
import asyncio
import websockets
import json
import numpy as np
import requests
import base64
import matplotlib.pyplot as plt
import matplotlib.patches as patches
import plotly.graph_objects as go
import plotly.express as px
from plotly.subplots import make_subplots
import pandas as pd
import time
import threading
from pathlib import Path
import tempfile
import wave
import io
from datetime import datetime

# Page configuration
st.set_page_config(
    page_title="SoundDitect - éŸ³å£°ç•°å¸¸æ¤œçŸ¥",
    page_icon="ğŸµ",
    layout="wide",
    initial_sidebar_state="expanded"
)

# Initialize session state
if 'mode' not in st.session_state:
    st.session_state.mode = None
if 'is_recording' not in st.session_state:
    st.session_state.is_recording = False
if 'detection_history' not in st.session_state:
    st.session_state.detection_history = []
if 'offline_results' not in st.session_state:
    st.session_state.offline_results = None
if 'connection_status' not in st.session_state:
    st.session_state.connection_status = "disconnected"

# Configuration
BACKEND_URL = "http://localhost:8000"
WEBSOCKET_URL = "ws://localhost:8000/ws"

# Helper functions
def reset_mode():
    """Reset to mode selection"""
    st.session_state.mode = None
    st.session_state.is_recording = False
    st.session_state.offline_results = None
    st.rerun()

def check_backend_status():
    """Check if the FastAPI backend is running"""
    try:
        response = requests.get(f"{BACKEND_URL}/", timeout=5)
        return response.status_code == 200
    except:
        return False

def process_offline_audio(audio_data, sample_rate, duration):
    """Process audio data for offline analysis"""
    try:
        # Convert audio to base64
        audio_bytes = (audio_data * 32767).astype(np.int16).tobytes()
        audio_base64 = base64.b64encode(audio_bytes).decode('utf-8')
        
        # Send to backend for batch analysis
        response = requests.post(
            f"{BACKEND_URL}/api/analyze_batch",
            json={
                "audio_data": audio_base64,
                "sample_rate": sample_rate,
                "duration": duration
            },
            timeout=30
        )
        
        if response.status_code == 200:
            return response.json()
        else:
            st.error(f"Backend error: {response.status_code}")
            return None
    except Exception as e:
        st.error(f"Analysis error: {e}")
        return None

# Header
st.markdown("""
<div style="text-align: center; padding: 2rem; background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); border-radius: 15px; margin-bottom: 2rem;">
    <h1 style="color: white; margin: 0; font-size: 3rem;">ğŸµ SoundDitect</h1>
    <p style="color: rgba(255,255,255,0.9); margin: 0; font-size: 1.2rem;">ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ éŸ³å£°ç•°å¸¸æ¤œçŸ¥ã‚·ã‚¹ãƒ†ãƒ </p>
</div>
""", unsafe_allow_html=True)

# Check backend status
backend_status = check_backend_status()
if not backend_status:
    st.error("ğŸš¨ FastAPI backend is not running. Please start the backend server first.")
    st.code("python run_server.py")
    st.stop()

# Sidebar - Mode Selection
st.sidebar.title("ğŸ¯ å‹•ä½œãƒ¢ãƒ¼ãƒ‰é¸æŠ")

if st.session_state.mode is None:
    # Mode selection
    st.sidebar.markdown("### å‹•ä½œãƒ¢ãƒ¼ãƒ‰ã‚’é¸æŠã—ã¦ãã ã•ã„")
    
    if st.sidebar.button("âš¡ ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ãƒ¢ãƒ¼ãƒ‰", use_container_width=True):
        st.session_state.mode = "realtime"
        st.rerun()
    
    st.sidebar.info("éŒ²éŸ³ä¸­ã«ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ã§éŸ³å£°ã‚’åˆ¤å®šã—ã¾ã™ã€‚é«˜æ€§èƒ½ãªPCã«é©ã—ã¦ã„ã¾ã™ã€‚")
    
    if st.sidebar.button("ğŸ“Š ã‚ªãƒ•ãƒ©ã‚¤ãƒ³ãƒ¢ãƒ¼ãƒ‰", use_container_width=True):
        st.session_state.mode = "offline"
        st.rerun()
    
    st.sidebar.info("éŒ²éŸ³å®Œäº†å¾Œã«ä¸€æ‹¬ã§åˆ†æã—ã¾ã™ã€‚è©³ç´°ãªåˆ†æçµæœã¨æ³¢å½¢è¡¨ç¤ºãŒåˆ©ç”¨ã§ãã¾ã™ã€‚")
    
else:
    # Mode selected - show back button and mode info
    mode_info = "âš¡ ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ãƒ¢ãƒ¼ãƒ‰" if st.session_state.mode == "realtime" else "ğŸ“Š ã‚ªãƒ•ãƒ©ã‚¤ãƒ³ãƒ¢ãƒ¼ãƒ‰"
    st.sidebar.success(f"é¸æŠä¸­: {mode_info}")
    
    if st.sidebar.button("â† ãƒ¢ãƒ¼ãƒ‰é¸æŠã«æˆ»ã‚‹", use_container_width=True):
        reset_mode()

# Main content based on selected mode
if st.session_state.mode is None:
    # Mode selection page
    st.markdown("## ğŸ¯ å‹•ä½œãƒ¢ãƒ¼ãƒ‰é¸æŠ")
    
    col1, col2 = st.columns(2)
    
    with col1:
        st.markdown("""
        <div style="border: 3px solid #4299e1; border-radius: 15px; padding: 2rem; text-align: center; height: 300px; display: flex; flex-direction: column; justify-content: center;">
            <div style="font-size: 4rem; margin-bottom: 1rem;">âš¡</div>
            <h3>ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ãƒ¢ãƒ¼ãƒ‰</h3>
            <p>éŒ²éŸ³ä¸­ã«ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ã§éŸ³å£°ã‚’åˆ¤å®šã—ã¾ã™ã€‚é«˜æ€§èƒ½ãªPCã«é©ã—ã¦ã„ã¾ã™ã€‚</p>
            <button style="background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); color: white; border: none; padding: 12px 24px; border-radius: 25px; font-weight: 600; cursor: pointer;">é¸æŠ</button>
        </div>
        """, unsafe_allow_html=True)
        
        if st.button("ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ãƒ¢ãƒ¼ãƒ‰ã‚’é¸æŠ", key="realtime_main", use_container_width=True):
            st.session_state.mode = "realtime"
            st.rerun()
    
    with col2:
        st.markdown("""
        <div style="border: 3px solid #4299e1; border-radius: 15px; padding: 2rem; text-align: center; height: 300px; display: flex; flex-direction: column; justify-content: center;">
            <div style="font-size: 4rem; margin-bottom: 1rem;">ğŸ“Š</div>
            <h3>ã‚ªãƒ•ãƒ©ã‚¤ãƒ³ãƒ¢ãƒ¼ãƒ‰</h3>
            <p>éŒ²éŸ³å®Œäº†å¾Œã«ä¸€æ‹¬ã§åˆ†æã—ã¾ã™ã€‚è©³ç´°ãªåˆ†æçµæœã¨æ³¢å½¢è¡¨ç¤ºãŒåˆ©ç”¨ã§ãã¾ã™ã€‚</p>
            <button style="background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); color: white; border: none; padding: 12px 24px; border-radius: 25px; font-weight: 600; cursor: pointer;">é¸æŠ</button>
        </div>
        """, unsafe_allow_html=True)
        
        if st.button("ã‚ªãƒ•ãƒ©ã‚¤ãƒ³ãƒ¢ãƒ¼ãƒ‰ã‚’é¸æŠ", key="offline_main", use_container_width=True):
            st.session_state.mode = "offline"
            st.rerun()

elif st.session_state.mode == "realtime":
    # Real-time mode interface
    st.markdown("## âš¡ ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ãƒ¢ãƒ¼ãƒ‰")
    
    # Connection status
    col1, col2, col3 = st.columns([2, 1, 1])
    with col1:
        if st.session_state.connection_status == "connected":
            st.success("ğŸŸ¢ ã‚µãƒ¼ãƒãƒ¼ã«æ¥ç¶šæ¸ˆã¿")
        else:
            st.warning("ğŸŸ¡ ã‚µãƒ¼ãƒãƒ¼ã«æ¥ç¶šä¸­...")
    
    with col2:
        if st.button("ğŸ”„ å†æ¥ç¶š", use_container_width=True):
            st.session_state.connection_status = "connecting"
            st.rerun()
    
    with col3:
        sensitivity = st.slider("æ„Ÿåº¦", 0.1, 1.0, 0.5, 0.1)
    
    # Recording controls
    st.markdown("### ğŸ¤ éŒ²éŸ³åˆ¶å¾¡")
    col1, col2 = st.columns(2)
    
    with col1:
        if not st.session_state.is_recording:
            if st.button("ğŸ¤ éŒ²éŸ³é–‹å§‹", type="primary", use_container_width=True):
                st.session_state.is_recording = True
                st.rerun()
        else:
            if st.button("â¹ï¸ éŒ²éŸ³åœæ­¢", type="secondary", use_container_width=True):
                st.session_state.is_recording = False
                st.rerun()
    
    # Detection results
    if st.session_state.is_recording:
        st.markdown("### ğŸ“Š æ¤œçŸ¥çµæœ")
        
        # Placeholder for real-time detection results
        detection_placeholder = st.empty()
        
        # Simulate real-time detection (in a real implementation, this would use WebSocket)
        with detection_placeholder.container():
            col1, col2, col3 = st.columns(3)
            
            with col1:
                st.metric("ç¾åœ¨ã®çŠ¶æ…‹", "æ­£å¸¸", delta="ä¿¡é ¼åº¦: 0.87")
            
            with col2:
                st.metric("æ¤œçŸ¥å›æ•°", len(st.session_state.detection_history))
            
            with col3:
                st.metric("ç•°å¸¸æ¤œçŸ¥", "0å›")
        
        # Audio visualization placeholder
        st.markdown("### ğŸ¨ éŸ³å£°å¯è¦–åŒ–")
        
        # Generate sample waveform for demonstration
        time_points = np.linspace(0, 1, 1000)
        waveform = np.sin(2 * np.pi * 5 * time_points) * np.exp(-time_points)
        
        fig = go.Figure()
        fig.add_trace(go.Scatter(x=time_points, y=waveform, mode='lines', name='éŸ³å£°æ³¢å½¢'))
        fig.update_layout(
            title="ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ éŸ³å£°æ³¢å½¢",
            xaxis_title="æ™‚é–“ (ç§’)",
            yaxis_title="æŒ¯å¹…",
            height=300
        )
        st.plotly_chart(fig, use_container_width=True)
        
        # Volume meter
        volume_level = 0.7  # Simulated volume
        st.progress(volume_level, text=f"éŸ³é‡ãƒ¬ãƒ™ãƒ«: {volume_level:.2f}")
        
        # Auto-refresh for real-time updates
        time.sleep(1)
        st.rerun()
    
    else:
        st.info("ğŸ“ éŒ²éŸ³ãƒœã‚¿ãƒ³ã‚’æŠ¼ã—ã¦é–‹å§‹ã—ã¦ãã ã•ã„")

elif st.session_state.mode == "offline":
    # Offline mode interface
    st.markdown("## ğŸ“Š ã‚ªãƒ•ãƒ©ã‚¤ãƒ³ãƒ¢ãƒ¼ãƒ‰")
    
    # Settings
    col1, col2 = st.columns([1, 2])
    with col1:
        recording_duration = st.slider("éŒ²éŸ³æ™‚é–“ (ç§’)", 5, 60, 30, 5)
    
    with col2:
        st.info(f"ğŸ“ {recording_duration}ç§’é–“éŒ²éŸ³ã—ã¦åˆ†æã—ã¾ã™")
    
    # Recording controls
    st.markdown("### ğŸ¤ éŒ²éŸ³åˆ¶å¾¡")
    col1, col2 = st.columns(2)
    
    with col1:
        if not st.session_state.is_recording:
            if st.button(f"ğŸ¤ éŒ²éŸ³é–‹å§‹ ({recording_duration}ç§’)", type="primary", use_container_width=True):
                st.session_state.is_recording = True
                st.session_state.offline_results = None
                st.rerun()
        else:
            if st.button("â¹ï¸ éŒ²éŸ³åœæ­¢", type="secondary", use_container_width=True):
                st.session_state.is_recording = False
                st.rerun()
    
    # Recording status and processing
    if st.session_state.is_recording:
        st.markdown("### â³ éŒ²éŸ³ä¸­...")
        
        # Progress bar for recording
        progress_bar = st.progress(0)
        status_text = st.empty()
        
        # Simulate recording progress
        for i in range(recording_duration):
            progress = (i + 1) / recording_duration
            progress_bar.progress(progress)
            status_text.text(f"éŒ²éŸ³ä¸­... {i+1}/{recording_duration}ç§’")
            time.sleep(1)
        
        # Processing
        status_text.text("åˆ†æä¸­...")
        progress_bar.progress(1.0)
        
        # Simulate audio data and processing
        sample_rate = 44100
        duration = recording_duration
        t = np.linspace(0, duration, int(sample_rate * duration))
        # Generate sample audio data (in real implementation, this would be actual recorded audio)
        audio_data = np.sin(2 * np.pi * 440 * t) + 0.1 * np.random.randn(len(t))
        
        # Process the audio
        with st.spinner("AIåˆ†æä¸­..."):
            # Simulate processing delay
            time.sleep(2)
            
            # Generate mock results (in real implementation, call process_offline_audio)
            results = {
                "summary": {
                    "total_duration": duration,
                    "ok_count": duration - 2,
                    "ng_count": 2,
                    "average_confidence": 0.85
                },
                "results": [
                    {
                        "time": i,
                        "prediction": 1 if i in [10, 25] else 0,  # Mock anomalies at 10s and 25s
                        "confidence": 0.9 if i in [10, 25] else np.random.uniform(0.7, 0.95),
                        "status": "NG" if i in [10, 25] else "OK"
                    }
                    for i in range(duration)
                ],
                "waveform_data": [audio_data[i*1000:(i+1)*1000].tolist() for i in range(duration)]
            }
        
        st.session_state.offline_results = results
        st.session_state.is_recording = False
        st.rerun()
    
    # Display offline results
    if st.session_state.offline_results:
        st.markdown("### ğŸ“Š åˆ†æçµæœ")
        
        results = st.session_state.offline_results
        
        # Summary statistics
        col1, col2, col3, col4 = st.columns(4)
        with col1:
            st.metric("ç·éŒ²éŸ³æ™‚é–“", f"{results['summary']['total_duration']:.1f}ç§’")
        with col2:
            st.metric("OKåˆ¤å®š", results['summary']['ok_count'], delta_color="normal")
        with col3:
            st.metric("NGåˆ¤å®š", results['summary']['ng_count'], delta_color="inverse")
        with col4:
            st.metric("å¹³å‡ä¿¡é ¼åº¦", f"{results['summary']['average_confidence']:.3f}")
        
        # Waveform with judgment overlays
        st.markdown("### ğŸ¨ æ³¢å½¢åˆ†æ")
        
        # Create waveform visualization
        time_points = np.linspace(0, results['summary']['total_duration'], len(results['waveform_data']) * 1000)
        waveform = np.concatenate([np.array(segment) for segment in results['waveform_data']])
        
        fig = make_subplots(rows=2, cols=1, 
                           subplot_titles=("éŸ³å£°æ³¢å½¢", "åˆ¤å®šçµæœ"),
                           vertical_spacing=0.1,
                           row_heights=[0.7, 0.3])
        
        # Waveform
        fig.add_trace(
            go.Scatter(x=time_points, y=waveform, mode='lines', name='éŸ³å£°æ³¢å½¢', line=dict(color='blue')),
            row=1, col=1
        )
        
        # Judgment overlays
        for result in results['results']:
            color = 'red' if result['prediction'] == 1 else 'green'
            alpha = result['confidence'] * 0.7
            
            fig.add_vrect(
                x0=result['time'], x1=result['time'] + 1,
                fillcolor=color, opacity=alpha,
                row=1, col=1
            )
        
        # Judgment timeline
        times = [r['time'] for r in results['results']]
        predictions = [r['prediction'] for r in results['results']]
        confidences = [r['confidence'] for r in results['results']]
        
        colors = ['red' if p == 1 else 'green' for p in predictions]
        
        fig.add_trace(
            go.Scatter(x=times, y=predictions, mode='markers', 
                      marker=dict(size=10, color=colors, opacity=0.8),
                      name='åˆ¤å®šçµæœ',
                      hovertemplate='æ™‚é–“: %{x}ç§’<br>åˆ¤å®š: %{text}<br>ä¿¡é ¼åº¦: %{customdata:.3f}<extra></extra>',
                      text=[r['status'] for r in results['results']],
                      customdata=confidences),
            row=2, col=1
        )
        
        fig.update_layout(height=600, title="è©³ç´°æ³¢å½¢åˆ†æ")
        fig.update_xaxes(title_text="æ™‚é–“ (ç§’)", row=2, col=1)
        fig.update_yaxes(title_text="æŒ¯å¹…", row=1, col=1)
        fig.update_yaxes(title_text="åˆ¤å®š (0=OK, 1=NG)", row=2, col=1)
        
        st.plotly_chart(fig, use_container_width=True)
        
        # Detailed results table
        st.markdown("### ğŸ“‹ è©³ç´°çµæœ")
        
        df = pd.DataFrame(results['results'])
        df['åˆ¤å®š'] = df['prediction'].map({0: 'OK', 1: 'NG'})
        df['æ™‚é–“'] = df['time'].astype(str) + 'ç§’'
        df['ä¿¡é ¼åº¦'] = df['confidence'].round(3)
        
        # Color-code the dataframe
        def highlight_ng(row):
            if row['prediction'] == 1:
                return ['background-color: #fed7d7'] * len(row)
            else:
                return ['background-color: #c6f6d5'] * len(row)
        
        st.dataframe(
            df[['æ™‚é–“', 'åˆ¤å®š', 'ä¿¡é ¼åº¦', 'status']].style.apply(highlight_ng, axis=1),
            use_container_width=True,
            height=400
        )
        
        # Download results
        if st.button("ğŸ“¥ çµæœã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰", use_container_width=True):
            # Convert results to downloadable format
            results_json = json.dumps(results, indent=2, ensure_ascii=False)
            st.download_button(
                label="JSONå½¢å¼ã§ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
                data=results_json,
                file_name=f"soundditect_results_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json",
                mime="application/json"
            )

# Sidebar - System Status
st.sidebar.markdown("---")
st.sidebar.markdown("### ğŸ“Š ã‚·ã‚¹ãƒ†ãƒ çŠ¶æ…‹")
st.sidebar.success("âœ… Backendæ¥ç¶šæ¸ˆã¿")
st.sidebar.info(f"ğŸ• {datetime.now().strftime('%H:%M:%S')}")

if st.session_state.detection_history:
    st.sidebar.metric("æ¤œçŸ¥å±¥æ­´", len(st.session_state.detection_history))

# Footer
st.markdown("---")
st.markdown("""
<div style="text-align: center; color: #718096; font-size: 0.9rem;">
    SoundDitect v2.0 - Streamlit Edition | Real-time Sound Anomaly Detection System
</div>
""", unsafe_allow_html=True)